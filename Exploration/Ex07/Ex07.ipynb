{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93936ba-ff0d-45fd-9f29-faa009851fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16873f7-3f28-43fa-8444-9648c9bb1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 데이터 수집하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21812e4d-33cd-43a4-95d9-5719f603bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750d6614-db57-445d-9d14-cd23a903df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "# 4. 트랜스포머의 입력 이해하기\n",
    "# 먼저 앞으로 진행하면서 필요한 패키지를 임포트하겠습니다!\n",
    "!pip install sentencepiece\n",
    "# sentencepiece 는 구글이 개발한 오픈소스 토크나이저로, 텍스트를 단어, 형태소 또는 문맥에 따라 의미 있는 단위(subword)로 분리해줍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676a373a-c745-4ccb-aa19-0a4a15813bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import sentencepiece as spm\n",
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db9f3d5-3566-42d6-93f8-4968072acc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/jovyan/work/AIFFEL_quest_rs/Exploration/Ex07\n",
      "파일이 성공적으로 로드되었습니다. 데이터 크기: (11823, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 현재 작업 디렉토리 확인\n",
    "current_directory = os.getcwd()\n",
    "print(f\"현재 작업 디렉토리: {current_directory}\")\n",
    "# CSV 파일 로드\n",
    "file_path_attempt = '/home/jovyan/work//transformer_chatbot/data/ChatbotData.csv' # 이전에 입력했던 경로에 문제가 없다면 이 경로를 다시 사용해 보세요.\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path_attempt)\n",
    "    print(f\"파일이 성공적으로 로드되었습니다. 데이터 크기: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: {file_path_attempt} 경로에서 파일을 찾을 수 없습니다. 경로를 다시 확인해 주세요.\")\n",
    "    print(\"팁: 파일의 절대 경로를 사용하거나, os.getcwd()로 현재 작업 디렉토리를 확인하여 상대 경로를 정확히 맞춰주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50dbb7c-01ca-4714-aff4-a92e0ab14902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 'songys_chatbot.txt' 파일로 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 질문(Q)과 답변(A)을 합쳐서 텍스트 파일로 저장\n",
    "with open('songys_chatbot.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in df['Q']:\n",
    "        f.write(text + '\\n')\n",
    "    for text in df['A']:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "print(\"데이터가 'songys_chatbot.txt' 파일로 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1d46ce-5edb-4e4e-99be-2aade141c166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece 모델 학습이 완료되었습니다. 'chatbot_sp.model', 'chatbot_sp.vocab' 파일이 생성되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=songys_chatbot.txt --model_prefix=chatbot_sp --vocab_size=8000 --model_type=bpe --unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: songys_chatbot.txt\n",
      "  input_format: \n",
      "  model_prefix: chatbot_sp\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: songys_chatbot.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 23646 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=353449\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1085\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23646 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23646\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 21781\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3353 min_freq=42\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1031 size=20 all=16243 active=1813 piece=▁생\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=759 size=40 all=16956 active=2526 piece=▁너\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=572 size=60 all=17594 active=3164 piece=해보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=422 size=80 all=18261 active=3831 piece=나봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=352 size=100 all=18985 active=4555 piece=▁먹\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=352 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=296 size=120 all=19482 active=1470 piece=▁될\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=261 size=140 all=20011 active=1999 piece=▁진\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=225 size=160 all=20411 active=2399 piece=▁필요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=201 size=180 all=20741 active=2729 piece=▁짝남\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=200 all=21115 active=3103 piece=사랑\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=185 min_freq=31\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=220 all=21471 active=1386 piece=▁싫\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=154 size=240 all=21802 active=1717 piece=▁것도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=260 all=22127 active=2042 piece=지만\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=280 all=22508 active=2423 piece=까지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=300 all=22837 active=2752 piece=해서\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=129 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=320 all=23202 active=1451 piece=▁귀\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=340 all=23536 active=1785 piece=해주세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=360 all=23793 active=2042 piece=보는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=380 all=24076 active=2325 piece=▁쓰\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=400 all=24497 active=2746 piece=▁바랍니다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=98 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=420 all=24773 active=1501 piece=▁말해보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=440 all=25016 active=1744 piece=▁있었\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=460 all=25270 active=1998 piece=▁사람은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=480 all=25523 active=2251 piece=▁몰라요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=500 all=25761 active=2489 piece=▁운동\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=77 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=520 all=25965 active=1475 piece=▁처\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=540 all=26195 active=1705 piece=▁나한테\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=560 all=26458 active=1968 piece=▁질\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=580 all=26656 active=2166 piece=▁생각을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=600 all=26813 active=2323 piece=나봅니다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=620 all=27009 active=1513 piece=기를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=640 all=27280 active=1784 piece=▁난\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=660 all=27504 active=2008 piece=▁목\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=680 all=27686 active=2190 piece=▁나는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=700 all=27820 active=2324 piece=▁대화를\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=720 all=27968 active=1539 piece=동안\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=740 all=28120 active=1691 piece=▁말해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=760 all=28250 active=1821 piece=▁사랑의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=780 all=28429 active=2000 piece=▁몇\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=800 all=28531 active=2102 piece=▁빨\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=46 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=820 all=28637 active=1527 piece=신이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=840 all=28795 active=1685 piece=▁거라\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=860 all=28925 active=1815 piece=▁한번\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=880 all=28985 active=1875 piece=▁강\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=900 all=29143 active=2033 piece=▁욕\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=40 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=920 all=29254 active=1555 piece=▁면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=940 all=29428 active=1729 piece=다니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=960 all=29587 active=1888 piece=▁4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=980 all=29734 active=2035 piece=▁지금도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1000 all=29869 active=2170 piece=▁솔직\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=36 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1020 all=30029 active=1643 piece=▁것이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1040 all=30173 active=1787 piece=▁내려\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1060 all=30281 active=1895 piece=▁책\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1080 all=30430 active=2044 piece=▁능\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1100 all=30540 active=2154 piece=▁친구들\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1120 all=30612 active=1594 piece=▁않죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1140 all=30726 active=1708 piece=▁아는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1160 all=30828 active=1810 piece=하길\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1180 all=30926 active=1908 piece=▁원하는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1200 all=31034 active=2016 piece=▁되지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1220 all=31093 active=1611 piece=▁사랑했던\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1240 all=31195 active=1713 piece=▁죽을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1260 all=31219 active=1737 piece=▁숨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1280 all=31316 active=1834 piece=▁월급\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1300 all=31377 active=1895 piece=▁이별의\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1320 all=31504 active=1695 piece=이가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1340 all=31613 active=1804 piece=▁약속\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1360 all=31667 active=1858 piece=▁남자애가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1380 all=31759 active=1950 piece=▁미치\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1400 all=31833 active=2024 piece=▁알려줘\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1420 all=31882 active=1639 piece=▁먹을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1440 all=32002 active=1759 piece=▁못하는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1460 all=32067 active=1824 piece=▁곳이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1480 all=32115 active=1872 piece=▁해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1500 all=32174 active=1931 piece=▁않는다면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1520 all=32310 active=1744 piece=▁기본\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1540 all=32391 active=1825 piece=드려요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1560 all=32420 active=1854 piece=▁자연스러운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1580 all=32524 active=1958 piece=야할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1600 all=32634 active=2068 piece=▁집중\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1620 all=32670 active=1661 piece=▁준비가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1640 all=32702 active=1693 piece=치가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1660 all=32809 active=1800 piece=▁오빠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1680 all=32895 active=1886 piece=▁받았어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1700 all=32904 active=1895 piece=▁걷\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1720 all=33032 active=1765 piece=이요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1740 all=33119 active=1852 piece=▁애랑\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1760 all=33175 active=1908 piece=▁아름다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1780 all=33192 active=1925 piece=고생\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1800 all=33328 active=2061 piece=▁나타\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1820 all=33384 active=1719 piece=▁있고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1840 all=33436 active=1771 piece=▁한동안\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1860 all=33477 active=1812 piece=냐고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1880 all=33583 active=1918 piece=▁다니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1900 all=33636 active=1971 piece=▁없애\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1920 all=33714 active=1754 piece=▁SNS\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1940 all=33731 active=1771 piece=▁사랑하는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1960 all=33787 active=1827 piece=어서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1980 all=33876 active=1916 piece=▁되면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2000 all=33944 active=1984 piece=▁주지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2020 all=33983 active=1737 piece=▁안해도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2040 all=33992 active=1746 piece=▁좋아할까\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2060 all=34032 active=1786 piece=간이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2080 all=34183 active=1937 piece=▁나의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2100 all=34229 active=1983 piece=▁왔다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2120 all=34256 active=1737 piece=▁무서운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2140 all=34251 active=1732 piece=▁사랑해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2160 all=34239 active=1720 piece=▁이해해주세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2180 all=34312 active=1793 piece=방법\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2200 all=34434 active=1915 piece=▁남을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2220 all=34471 active=1757 piece=▁생활\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2240 all=34543 active=1829 piece=▁줘도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2260 all=34601 active=1887 piece=▁기념일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2280 all=34609 active=1895 piece=▁연애를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2300 all=34617 active=1903 piece=▁당당하게\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2320 all=34614 active=1728 piece=▁헤어졌는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2340 all=34677 active=1791 piece=렸어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2360 all=34751 active=1865 piece=▁다들\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2380 all=34779 active=1893 piece=▁신고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2400 all=34824 active=1938 piece=▁편한\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2420 all=34855 active=1773 piece=▁만난지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2440 all=34850 active=1768 piece=▁운동을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2460 all=34840 active=1758 piece=▁아니라면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2480 all=34825 active=1743 piece=▁적극적으로\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2500 all=34859 active=1777 piece=▁헬\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2520 all=34960 active=1843 piece=전환\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2540 all=34998 active=1881 piece=▁나쁘\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2560 all=35032 active=1915 piece=▁사용\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2580 all=35074 active=1957 piece=▁움직\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2600 all=35130 active=2013 piece=싶은데\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2620 all=35160 active=1779 piece=▁모습이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2640 all=35159 active=1778 piece=▁정리할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2660 all=35167 active=1786 piece=▁썸남이랑\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2680 all=35155 active=1774 piece=▁고백해보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2700 all=35194 active=1813 piece=갔어\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2720 all=35278 active=1840 piece=인이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2740 all=35328 active=1890 piece=▁다되\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2760 all=35371 active=1933 piece=▁보기\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2780 all=35410 active=1972 piece=▁이불\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2800 all=35457 active=2019 piece=이라니\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2820 all=35493 active=1802 piece=▁들어가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2840 all=35489 active=1798 piece=▁앞머리\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2860 all=35479 active=1788 piece=▁피곤해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2880 all=35487 active=1796 piece=▁스트레칭\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2900 all=35477 active=1786 piece=▁믿어보세요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2920 all=35509 active=1806 piece=▁띄\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2940 all=35585 active=1882 piece=렌타\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2960 all=35702 active=1999 piece=함을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2980 all=35735 active=2032 piece=▁데는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3000 all=35759 active=2056 piece=▁삶을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3020 all=35778 active=1807 piece=▁외모\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3040 all=35812 active=1841 piece=▁회원\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3060 all=35887 active=1916 piece=▁감정의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3080 all=35886 active=1915 piece=▁되는지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3100 all=35875 active=1904 piece=▁알아차\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3120 all=35870 active=1786 piece=▁재미가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3140 all=35863 active=1779 piece=▁기다리면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3160 all=35859 active=1775 piece=▁있었으면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3180 all=35852 active=1768 piece=▁감사합니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3200 all=35832 active=1748 piece=▁A\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3220 all=35870 active=1828 piece=겹살\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3240 all=35938 active=1896 piece=이스\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3260 all=35988 active=1946 piece=▁그립\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3280 all=36016 active=1974 piece=▁로또\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3300 all=36050 active=2008 piece=▁사면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3320 all=36087 active=1840 piece=▁어이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3340 all=36113 active=1866 piece=▁적금\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3360 all=36147 active=1900 piece=▁한숨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3380 all=36214 active=1967 piece=을텐데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3400 all=36240 active=1993 piece=▁끝났어\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3420 all=36235 active=1807 piece=▁마시면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3440 all=36225 active=1797 piece=▁성공할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3460 all=36217 active=1789 piece=▁우연히\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3480 all=36212 active=1784 piece=▁회사와\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3500 all=36223 active=1795 piece=▁드릴게요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3520 all=36216 active=1805 piece=▁좋아하나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3540 all=36205 active=1794 piece=▁사랑이네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3560 all=36191 active=1780 piece=▁끓\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3580 all=36232 active=1821 piece=▁흥\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3600 all=36287 active=1876 piece=렸을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3620 all=36363 active=1886 piece=장에\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3640 all=36388 active=1911 piece=▁건조\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3660 all=36402 active=1925 piece=▁노는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3680 all=36425 active=1948 piece=▁발목\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3700 all=36445 active=1968 piece=▁쓰지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3720 all=36457 active=1835 piece=▁있다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3740 all=36474 active=1852 piece=▁충격\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3760 all=36519 active=1897 piece=에서도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3780 all=36561 active=1939 piece=▁경험이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3800 all=36545 active=1923 piece=▁될지도\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3820 all=36543 active=1826 piece=▁번호를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3840 all=36533 active=1816 piece=▁아니고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3860 all=36527 active=1810 piece=▁운명이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3880 all=36528 active=1811 piece=▁처음에\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3900 all=36523 active=1806 piece=하니까요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3920 all=36515 active=1814 piece=▁사람인데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3940 all=36501 active=1800 piece=▁잡으세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3960 all=36496 active=1795 piece=▁가능합니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3980 all=36479 active=1778 piece=▁이야기해도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4000 all=36472 active=1771 piece=▁광\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4020 all=36496 active=1845 piece=거같\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4040 all=36558 active=1907 piece=라서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4060 all=36619 active=1968 piece=작정\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4080 all=36673 active=2022 piece=해를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4100 all=36692 active=2041 piece=▁고장\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4120 all=36713 active=1854 piece=▁단순\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4140 all=36735 active=1876 piece=▁맞춤\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4160 all=36750 active=1891 piece=▁삶의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4180 all=36764 active=1905 piece=▁쌩얼\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4200 all=36780 active=1921 piece=▁온다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4220 all=36793 active=1852 piece=▁입이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4240 all=36806 active=1865 piece=▁진지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4260 all=36839 active=1898 piece=▁티가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4280 all=36852 active=1911 piece=것같아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4300 all=36900 active=1959 piece=으려면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4320 all=36942 active=1883 piece=▁건가요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4340 all=36930 active=1871 piece=▁기대가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4360 all=36916 active=1857 piece=▁되겠지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4380 all=36906 active=1847 piece=▁무거워\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4400 all=36897 active=1838 piece=▁사람만\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4420 all=36881 active=1829 piece=▁아프고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4440 all=36868 active=1816 piece=▁의미가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4460 all=36863 active=1811 piece=▁중요할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4480 all=36859 active=1807 piece=▁터놓고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4500 all=36848 active=1796 piece=었겠네요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4520 all=36846 active=1838 piece=▁다가가는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4540 all=36836 active=1828 piece=▁생각나고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4560 all=36830 active=1822 piece=▁오토바이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4580 all=36814 active=1806 piece=▁처음에는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4600 all=36807 active=1799 piece=▁마음이라도\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4620 all=36790 active=1824 piece=▁안타깝네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4640 all=36773 active=1807 piece=▁시간이겠네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4660 all=36768 active=1802 piece=▁떡\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4680 all=36798 active=1832 piece=▁탓\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4700 all=36843 active=1877 piece=되기\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4720 all=36886 active=1883 piece=생각\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4740 all=36954 active=1951 piece=은거\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4760 all=36999 active=1996 piece=한번\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4780 all=37012 active=2009 piece=▁괜한\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4800 all=37031 active=2028 piece=▁낭만\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4820 all=37030 active=1849 piece=▁등산\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4840 all=37037 active=1856 piece=▁무한\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4860 all=37045 active=1864 piece=▁봄은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4880 all=37057 active=1876 piece=▁수업\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4900 all=37070 active=1889 piece=▁어깨\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4920 all=37089 active=1873 piece=▁작아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4940 all=37107 active=1891 piece=▁짧게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4960 all=37126 active=1910 piece=▁표정\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4980 all=37160 active=1944 piece=릅니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5000 all=37206 active=1990 piece=해졌어\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5020 all=37211 active=1862 piece=▁기다림\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5040 all=37205 active=1856 piece=▁뒤숭숭\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5060 all=37196 active=1847 piece=▁망했다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5080 all=37190 active=1841 piece=▁바뀌게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5100 all=37178 active=1829 piece=▁사라질\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5120 all=37167 active=1847 piece=▁아무말\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5140 all=37162 active=1842 piece=▁오빠가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5160 all=37151 active=1831 piece=▁주면서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5180 all=37144 active=1824 piece=▁친해질\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5200 all=37131 active=1811 piece=▁행운을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5220 all=37147 active=1873 piece=▁것입니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5240 all=37134 active=1860 piece=▁만나는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5260 all=37117 active=1843 piece=▁생각해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5280 all=37106 active=1832 piece=▁없겠지요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5300 all=37092 active=1818 piece=▁있을게요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5320 all=37076 active=1839 piece=▁짝사랑을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5340 all=37070 active=1833 piece=▁걱정된다고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5360 all=37050 active=1813 piece=▁상황이네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5380 all=37032 active=1795 piece=▁프라이버시\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5400 all=37020 active=1783 piece=▁갠\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5420 all=37043 active=1874 piece=���뻑\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5440 all=37056 active=1887 piece=가가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5460 all=37097 active=1928 piece=꺼야\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5480 all=37134 active=1965 piece=리죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5500 all=37162 active=1993 piece=뻑해\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5520 all=37196 active=1892 piece=영상\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5540 all=37239 active=1935 piece=질뻔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5560 all=37275 active=1971 piece=했나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5580 all=37287 active=1983 piece=▁걷고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5600 all=37288 active=1984 piece=▁끈을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5620 all=37285 active=1861 piece=▁누락\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5640 all=37286 active=1862 piece=▁드릴\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5660 all=37281 active=1857 piece=▁먹은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5680 all=37290 active=1866 piece=▁바꿨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5700 all=37303 active=1879 piece=▁부질\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5720 all=37324 active=1884 piece=▁소름\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5740 all=37339 active=1899 piece=▁안부\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5760 all=37343 active=1903 piece=▁오려\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5780 all=37344 active=1904 piece=▁음료\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5800 all=37360 active=1920 piece=▁적당\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5820 all=37372 active=1878 piece=▁지속\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5840 all=37387 active=1893 piece=▁축제\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5860 all=37389 active=1895 piece=▁패션\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5880 all=37397 active=1903 piece=▁환기\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5900 all=37415 active=1921 piece=보려고\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5920 all=37452 active=1904 piece=하려면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5940 all=37473 active=1925 piece=▁각질제\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5960 all=37458 active=1910 piece=▁곳까지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5980 all=37442 active=1894 piece=▁기울여\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6000 all=37433 active=1885 piece=▁노래는\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6020 all=37419 active=1858 piece=▁되어요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6040 all=37410 active=1849 piece=▁막히는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6060 all=37400 active=1839 piece=▁먹는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6080 all=37386 active=1825 piece=▁발전이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6100 all=37373 active=1812 piece=▁불편한\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6120 all=37359 active=1855 piece=▁선택이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6140 all=37353 active=1849 piece=▁식습관\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6160 all=37339 active=1835 piece=▁아픔도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6180 all=37330 active=1826 piece=▁어리석\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6200 all=37317 active=1813 piece=▁오니까\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6220 all=37306 active=1855 piece=▁이러는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6240 all=37298 active=1847 piece=▁있군요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6260 all=37294 active=1843 piece=▁정리해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6280 all=37278 active=1827 piece=▁지금의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6300 all=37263 active=1812 piece=▁콩깍지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6320 all=37245 active=1846 piece=▁해줄까\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6340 all=37238 active=1839 piece=받으세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6360 all=37259 active=1860 piece=▁간사하죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6380 all=37248 active=1849 piece=▁괜찮을거\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6400 all=37232 active=1833 piece=▁너무너무\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6420 all=37215 active=1844 piece=▁들리려고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6440 all=37196 active=1825 piece=▁물어봐도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6460 all=37184 active=1813 piece=▁사람이야\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6480 all=37167 active=1796 piece=▁스테이크\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6500 all=37152 active=1781 piece=▁않겠네요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6520 all=37135 active=1841 piece=▁오래오래\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6540 all=37119 active=1825 piece=▁이해하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6560 all=37106 active=1812 piece=▁존중하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6580 all=37090 active=1796 piece=▁컴퓨터가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6600 all=37072 active=1778 piece=▁핸드폰이\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6620 all=37063 active=1845 piece=▁갑��스러운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6640 all=37047 active=1829 piece=▁다행이네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6660 all=37027 active=1809 piece=▁병원가세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6680 all=37009 active=1791 piece=▁소개팅하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6700 all=36989 active=1771 piece=▁연락해보는\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6720 all=36970 active=1831 piece=▁정정기간을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6740 all=36953 active=1814 piece=▁하우스웨딩\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6760 all=36935 active=1796 piece=▁부딪혔나봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6780 all=36915 active=1776 piece=▁힘드시겠어요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6800 all=36912 active=1773 piece=▁넋\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6820 all=36928 active=1861 piece=▁앱\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6840 all=36939 active=1872 piece=▁팩\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6860 all=36959 active=1892 piece=계절\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6880 all=36986 active=1919 piece=낭비\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6900 all=37008 active=1941 piece=동차\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: chatbot_sp.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: chatbot_sp.vocab\n"
     ]
    }
   ],
   "source": [
    "# 2. SentencePiece 모델 학습\n",
    "\n",
    "\n",
    "# SentencePiece 모델 학습\n",
    "spm.SentencePieceTrainer.train(\n",
    "    '--input=songys_chatbot.txt '  # 위에서 생성한 입력 텍스트 파일\n",
    "    '--model_prefix=chatbot_sp '  # 생성될 모델 파일의 접두사\n",
    "    '--vocab_size=8000 '          # 어휘 크기 설정\n",
    "    '--model_type=bpe '           # BPE(Byte-Pair Encoding) 모델 사용\n",
    "    '--unk_id=0 '\n",
    "    '--pad_id=1 '\n",
    "    '--bos_id=2 '                 # [START] 토큰 ID\n",
    "    '--eos_id=3'                  # [END] 토큰 ID\n",
    ")\n",
    "\n",
    "print(\"SentencePiece 모델 학습이 완료되었습니다. 'chatbot_sp.model', 'chatbot_sp.vocab' 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb8d16-d61e-480b-8a42-c4ec4d22ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. SentencePiece 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43aee597-6714-41d2-b995-e2e93b007d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePiece 모델 로드\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('chatbot_sp.model')\n",
    "\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, df, sp, max_seq_length):\n",
    "        self.df = df\n",
    "        self.sp = sp\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.q_tokens = [self.sp.encode_as_ids(q) for q in df['Q']]\n",
    "        self.a_tokens = [self.sp.encode_as_ids(a) for a in df['A']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 질문 토큰화 및 특수 토큰 추가 (입력)\n",
    "        q_ids = [self.sp.bos_id()] + self.q_tokens[idx] + [self.sp.eos_id()]\n",
    "        \n",
    "        # 답변 토큰화 및 특수 토큰 추가 (출력)\n",
    "        a_ids = [self.sp.bos_id()] + self.a_tokens[idx] + [self.sp.eos_id()]\n",
    "        \n",
    "        # 패딩\n",
    "        q_padded = self.pad_sequence(q_ids)\n",
    "        a_padded = self.pad_sequence(a_ids)\n",
    "\n",
    "        return torch.tensor(q_padded, dtype=torch.long), torch.tensor(a_padded, dtype=torch.long)\n",
    "    \n",
    "    def pad_sequence(self, sequence):\n",
    "        if len(sequence) >= self.max_seq_length:\n",
    "            return sequence[:self.max_seq_length]\n",
    "        else:\n",
    "            padding = [self.sp.pad_id()] * (self.max_seq_length - len(sequence))\n",
    "            return sequence + padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc64149-b3c3-4584-97b5-4306a499a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 전처리 함수 호출 한국어 전처리를 통해 학습 데이터셋을 구축하였다.\n",
    "def preprocess_data(dataframe, sp_processor, max_len):\n",
    "    \"\"\"\n",
    "    데이터프레임과 SentencePiece 프로세서를 사용하여\n",
    "    전처리된 PyTorch Dataset을 생성합니다.\n",
    "    \"\"\"\n",
    "    return ChatbotDataset(dataframe, sp_processor, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2161e19-6cc3-43a3-be01-65b1db1f25fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 텐서 (첫 번째 샘플): torch.Size([50])\n",
      "답변 텐서 (첫 번째 샘플): torch.Size([50])\n",
      "질문 토큰 ID: tensor([   2, 5566, 6957, 3207, 7063,    3,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n",
      "답변 토큰 ID: tensor([   2, 4489,  211, 5936, 6916,    3,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n",
      "질문 원문: 12시 땡!\n",
      "답변 원문: 하루가 또 가네요.\n"
     ]
    }
   ],
   "source": [
    "# 에러가 난다 다시 코딩을 바꾼다\n",
    "# 클래스 외부에서 토큰 ID를 디코딩하려면, self 대신에 위에서 생성한 sp 객체와 dataset 객체를 직접 사용해야 합니다.\n",
    "# Dataset 클래스 인스턴스 생성\n",
    "max_len = 50 \n",
    "dataset = ChatbotDataset(df, sp, max_len)\n",
    "\n",
    "# 데이터셋의 첫 번째 항목 확인\n",
    "q_tensor, a_tensor = dataset[0]\n",
    "print(\"질문 텐서 (첫 번째 샘플):\", q_tensor.shape)\n",
    "print(\"답변 텐서 (첫 번째 샘플):\", a_tensor.shape)\n",
    "print(\"질문 토큰 ID:\", q_tensor)\n",
    "print(\"답변 토큰 ID:\", a_tensor)\n",
    "\n",
    "# 토큰 ID를 다시 문장으로 변환하여 확인\n",
    "# 'self' 대신에 위에서 생성한 'sp' 객체를 직접 사용합니다.\n",
    "q_sentence = sp.decode_ids(q_tensor.tolist())\n",
    "a_sentence = sp.decode_ids(a_tensor.tolist())\n",
    "\n",
    "print(\"질문 원문:\", q_sentence)\n",
    "print(\"답변 원문:\", a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40497616-6d32-4fab-9840-ab1a3ea87e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 토큰 ID 출력 ###\n",
      "질문 텐서 (토큰 ID): tensor([   2, 5566, 6957, 3207, 7063,    3,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n",
      "답변 텐서 (토큰 ID): tensor([   2, 4489,  211, 5936, 6916,    3,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1])\n",
      "\n",
      "### 디코딩된 문장 출력 ###\n",
      "질문 (디코딩): 12시 땡!\n",
      "답변 (디코딩): 하루가 또 가네요.\n",
      "\n",
      "### 원본 문장과 비교 ###\n",
      "원본 질문: 12시 땡!\n",
      "원본 답변: 하루가 또 가네요.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 첫 번째 샘플을 가져와 테스트합니다.\n",
    "q_tensor, a_tensor = dataset[0]\n",
    "\n",
    "# 토큰 ID 텐서 출력\n",
    "print(\"### 토큰 ID 출력 ###\")\n",
    "print(f\"질문 텐서 (토큰 ID): {q_tensor}\")\n",
    "print(f\"답변 텐서 (토큰 ID): {a_tensor}\")\n",
    "\n",
    "# 토큰 ID를 다시 문장으로 디코딩하여 확인\n",
    "decoded_q = sp.decode_ids(q_tensor.tolist())\n",
    "decoded_a = sp.decode_ids(a_tensor.tolist())\n",
    "print(\"\\n### 디코딩된 문장 출력 ###\")\n",
    "print(f\"질문 (디코딩): {decoded_q}\")\n",
    "print(f\"답변 (디코딩): {decoded_a}\")\n",
    "\n",
    "# 원본 문장과 비교하여 전처리가 잘 되었는지 확인\n",
    "print(\"\\n### 원본 문장과 비교 ###\")\n",
    "print(f\"원본 질문: {df['Q'].iloc[0]}\")\n",
    "print(f\"원본 답변: {df['A'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ca7321-aac9-48bf-bf80-4598d0d5a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치의 질문 텐서 크기: torch.Size([64, 50])\n",
      "첫 번째 배치의 답변 텐서 크기: torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기 설정 (예: 64)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# DataLoader 생성\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 첫 번째 배치 확인\n",
    "# DataLoader를 사용하면 데이터가 텐서 묶음으로 반환됩니다.\n",
    "for batch in train_dataloader:\n",
    "    q_batch, a_batch = batch\n",
    "    \n",
    "    print(\"첫 번째 배치의 질문 텐서 크기:\", q_batch.shape) # (BATCH_SIZE, max_len)\n",
    "    print(\"첫 번째 배치의 답변 텐서 크기:\", a_batch.shape) # (BATCH_SIZE, max_len)\n",
    "    \n",
    "    # 첫 번째 배치만 확인하고 루프 종료\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33c46a55-9b97-4e29-8376-de967276d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 모델 정의 및 학습하기 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.\n",
    "\n",
    "class TransformerModel(nn.Module): # 완성된 모델을 구현하기위해 변경\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 d_model=512,          # 임베딩 및 내부 표현 차원\n",
    "                 nhead=8,              # 멀티헤드 어텐션의 헤드 수\n",
    "                 num_encoder_layers=6, # 인코더 층 수\n",
    "                 num_decoder_layers=6, # 디코더 층 수\n",
    "                 dim_feedforward=2048, \n",
    "                 dropout=0.1):\n",
    "        \"\"\"\n",
    "        초기화 함수. 모델의 구성 요소를 정의합니다.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): 단어 집합의 크기. 모델이 예측할 수 있는 총 단어의 개수.\n",
    "            d_model (int): 모델의 임베딩 차원.\n",
    "            nhead (int): 어텐션 헤드의 개수.\n",
    "            num_encoder_layers (int): 인코더 스택의 레이어 수.\n",
    "            num_decoder_layers (int): 디코더 스택의 레이어 수.\n",
    "            dim_feedforward (int): 피드포워드 신경망의 은닉층 차원.\n",
    "            dropout (float): 드롭아웃 비율.\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # 임베딩 레이어와 위치 인코딩\n",
    "        # 1. 단어를 벡터로 변환하는 임베딩 레이어\n",
    "        # (vocab_size, d_model) 크기의 임베딩 테이블을 만듭니다.\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 2. 단어의 위치 정보를 추가하는 위치 인코딩 레이어\n",
    "        # 트랜스포머는 순서 정보를 직접 처리하지 못하므로, 이 레이어를 통해 위치 정보를 주입합니다.\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # 3. PyTorch의 내장된 핵심 트랜스포머 모듈 (인코더와 디코더)\n",
    "        # 트랜스포머 인코더와 디코더\n",
    "        # 이 모듈이 멀티-헤드 어텐션, 피드포워드 신경망 등 핵심 연산을 처리합니다.\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 4. 최종 출력 레이어\n",
    "        # 디코더의 출력을 받아, 단어 집합 크기(vocab_size)에 맞는 로짓(logit)을 생성합니다.\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        \"\"\"\n",
    "        순전파 함수. 입력(src)과 타겟(tgt)을 받아 출력을 계산합니다.\n",
    "        \n",
    "        Args:\n",
    "            src (Tensor): 인코더에 들어갈 소스(질문) 시퀀스 텐서.\n",
    "            tgt (Tensor): 디코더에 들어갈 타겟(답변) 시퀀스 텐서.\n",
    "            src_mask (Tensor): 소스 마스크 (패딩 토큰을 가리기 위함).\n",
    "            tgt_mask (Tensor): 타겟 마스크 (미래 토큰을 가리기 위함).\n",
    "            src_padding_mask (Tensor): 소스 패딩 마스크.\n",
    "            tgt_padding_mask (Tensor): 타겟 패딩 마스크.\n",
    "        \"\"\"\n",
    "        # 1. 입력 시퀀스에 임베딩과 위치 인코딩 적용\n",
    "        # 임베딩 값에 루트(d_model)을 곱하여 스케일링을 합니다.\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32))\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        # 2. 타겟 시퀀스에 임베딩과 위치 인코딩 적용\n",
    "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32))\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        # 3. 트랜스포머 모델에 입력\n",
    "        # 인코더와 디코더를 통과하여 출력을 생성합니다.\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n",
    "        # 4. 최종 출력 레이어를 통과시켜 다음 단어의 확률을 예측\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "# PositionalEncoding 클래스: 단어의 위치 정보를 인코딩합니다.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 텐서에 미리 계산된 위치 인코딩 값을 더해줍니다.\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bac25863-78de-4dcd-acf7-bb45fd30b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터\n",
    "VOCAB_SIZE = 8000 # SentencePiece 학습 시 설정한 어휘 크기\n",
    "D_MODEL = 512           # 모델의 임베딩 차원\n",
    "NHEADS = 8              # 멀티-헤드 어텐션의 헤드 수\n",
    "NUM_ENCODER_LAYERS = 6  # 인코더 레이어 수\n",
    "NUM_DECODER_LAYERS = 6  # 디코더 레이어 수\n",
    "DROPOUT = 0.1           # 드롭아웃 비율\n",
    "\n",
    "# GPU 사용 설정\n",
    "# CUDA를 사용할 수 있으면 'cuda'를, 아니면 'cpu'를 사용합니다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"현재 사용 가능한 장치: {device}\")\n",
    "\"\"\"\n",
    "# 수정 전\n",
    "# 모델 인스턴스 생성 및 장치로 이동\n",
    "model = TransformerModel(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    d_model=D_MODEL,\n",
    "    nhead=NHEADS,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\"\"\"\n",
    "# 수정 후\n",
    "model = TransformerModel(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    d_model=D_MODEL,\n",
    "    nhead=NHEADS,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "\n",
    "# 옵티마이저 정의: 모델의 파라미터(가중치)를 최적화(업데이트)하는 역할을 합니다.\n",
    "# Adam 옵티마이저는 학습률(lr)을 효율적으로 조절하여 성능을 높입니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 손실 함수 정의: 모델의 예측값과 실제 정답 간의 오차를 계산합니다.\n",
    "# CrossEntropyLoss는 분류 문제에 사용되며, ignore_index를 통해 패딩 토큰은 손실 계산에서 제외합니다.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfe3473f-159e-44e6-99ca-8d7034cf58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#어텐션 마스크 및 훈련/평가 함수 정의하기\n",
    "\"\"\"\n",
    "트랜스포머 모델은 다음 두 종류의 마스크를 사용합니다.\n",
    "\n",
    "소스 마스크 (src_mask): 인코더가 패딩(Padding) 토큰을 무시하도록 합니다.\n",
    "\n",
    "타겟 마스크 (tgt_mask): 디코더가 아직 생성되지 않은 미래의 토큰을 보지 못하도록 합니다.\n",
    "\n",
    "이러한 마스크는 학습 중 모델이 \"부정행위\"를 하지 않도록 방지하는 중요한 역할을 합니다. 아래 코드는 이 두 가지 마스크를 생성하는 함수를 정의합니다.\n",
    "\"\"\"\n",
    "def create_masks(src, tgt, sp):\n",
    "    # 인코더의 입력(src)에 대한 패딩 마스크 생성\n",
    "    # 패딩 토큰(sp.pad_id())의 위치를 True로 표시하여 어텐션 계산에서 제외합니다.\n",
    "    src_padding_mask = (src == sp.pad_id())\n",
    "\n",
    "    # 디코더의 입력(tgt)에 대한 미래 토큰 마스크 생성\n",
    "    # 디코더가 현재 단어 이후의 단어를 보지 못하도록 가립니다.\n",
    "    tgt_len = tgt.shape[1]\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
    "\n",
    "    # 디코더의 입력(tgt)에 대한 패딩 마스크 생성\n",
    "    tgt_padding_mask = (tgt == sp.pad_id())\n",
    "\n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# generate_masks 함수는 Ex07.ipynb 코드에서 다르게 구현될 수 있습니다.\n",
    "# 위 코드는 일반적인 PyTorch 구현 방식을 따르며,\n",
    "# Ex07.ipynb에서는 src_mask가 사용되지 않을 수 있습니다.\n",
    "# (torch.nn.Transformer에 기본적으로 내장되어 있기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a33bb4-5e1d-4540-8b81-45a79c8eb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. 훈련 함수 정의\n",
    "이제 DataLoader에서 데이터를 가져와 모델을 학습시킬 함수를 정의합니다. \n",
    "이 함수는 한 에폭(epoch) 동안의 훈련 과정을 담고 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec49db07-69c3-4a0a-b513-cbff6fd92711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device):\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # 타겟 시퀀스를 모델의 입력과 정답으로 분리합니다.\n",
    "        # 예: [START, 문장1, 문장2, END] -> 입력: [START, 문장1, 문장2], 정답: [문장1, 문장2, END]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # 마스크 생성\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "\n",
    "        # 옵티마이저 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 (Forward Pass): 모델에 입력과 마스크를 전달\n",
    "        # 주의: 여기서는 src_mask를 사용하지 않습니다. PyTorch의 nn.Transformer가 src_padding_mask로 대체할 수 있습니다.\n",
    "        predictions = model(src, tgt_input, src_mask=None, tgt_mask=tgt_mask,\n",
    "                            src_key_padding_mask=src_padding_mask,\n",
    "                            tgt_key_padding_mask=tgt_padding_mask)\n",
    "\n",
    "        # 손실 계산: 예측값과 정답 간의 오차를 계산\n",
    "        # view()를 사용하여 텐서를 1차원으로 펼친 후 손실 함수에 전달합니다.\n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "        # 역전파 (Backward Pass)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저로 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7258a33a-a442-4096-84dc-60348995e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 훈련 시작...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerModel.forward() got an unexpected keyword argument 'src_key_padding_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, dataloader, sp, device)\u001b[39m\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 순전파 (Forward Pass): 모델에 입력과 마스크를 전달\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 주의: 여기서는 src_mask를 사용하지 않습니다. PyTorch의 nn.Transformer가 src_padding_mask로 대체할 수 있습니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 손실 계산: 예측값과 정답 간의 오차를 계산\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# view()를 사용하여 텐서를 1차원으로 펼친 후 손실 함수에 전달합니다.\u001b[39;00m\n\u001b[32m     27\u001b[39m loss = criterion(predictions.view(-\u001b[32m1\u001b[39m, predictions.shape[-\u001b[32m1\u001b[39m]), tgt_output.reshape(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: TransformerModel.forward() got an unexpected keyword argument 'src_key_padding_mask'"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 루프 실행\n",
    "# 마지막으로, 위에서 정의한 함수들을 사용하여 모델을 여러 에폭 동안 훈련시키는 루프를 실행합니다.\n",
    "# 하이퍼파라미터 정의 (이전 단계에서 이미 정의한 경우 생략 가능)\n",
    "NUM_EPOCHS = 10 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# (이전 코드에서 정의된) model, optimizer, criterion, train_dataloader, sp, device 변수들이 필요합니다.\n",
    "\n",
    "# 훈련 시작\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf4a059c-7c14-494b-ba4e-0c89bfaea26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 훈련 시작...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, dataloader, sp, device)\u001b[39m\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 순전파 (Forward Pass)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# model.forward()의 인자 이름에 맞게 수정\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[32m     27\u001b[39m loss = criterion(predictions.view(-\u001b[32m1\u001b[39m, predictions.shape[-\u001b[32m1\u001b[39m]), tgt_output.reshape(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mTransformerModel.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m순전파 함수. 입력(src)과 타겟(tgt)을 받아 출력을 계산합니다.\u001b[39;00m\n\u001b[32m     55\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m \u001b[33;03m    tgt_padding_mask (Tensor): 타겟 패딩 마스크.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 1. 입력 시퀀스에 임베딩과 위치 인코딩 적용\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# 임베딩 값에 루트(d_model)을 곱하여 스케일링을 합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m src = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m * torch.sqrt(torch.tensor(\u001b[38;5;28mself\u001b[39m.transformer.d_model, dtype=torch.float32))\n\u001b[32m     67\u001b[39m src = \u001b[38;5;28mself\u001b[39m.positional_encoding(src)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# 2. 타겟 시퀀스에 임베딩과 위치 인코딩 적용\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# 수정된 train_epoch 함수\n",
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device):\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # 마스크 생성\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "\n",
    "        # 옵티마이저 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 (Forward Pass)\n",
    "        # model.forward()의 인자 이름에 맞게 수정\n",
    "        predictions = model(src, tgt_input,\n",
    "                            src_mask=None,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "        # 역전파 (Backward Pass)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저로 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 전체 훈련 루프\n",
    "NUM_EPOCHS = 10 \n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32dd2e23-88c4-4f49-b9c6-4c3e541079aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        # 1. 입력 시퀀스에 임베딩과 위치 인코딩 적용\n",
    "        # **수정된 부분:** torch.tensor()로 생성된 텐서를 .to(src.device)로 이동\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(src.device))\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        # 2. 타겟 시퀀스에 임베딩과 위치 인코딩 적용\n",
    "        # **수정된 부분:** torch.tensor()로 생성된 텐서를 .to(tgt.device)로 이동\n",
    "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(tgt.device))\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        # 3. 트랜스포머 모델에 입력\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n",
    "        \n",
    "        # 4. 최종 출력 레이어를 통과시켜 다음 단어의 확률을 예측\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8bb410d-4aff-4abc-9d94-441e0f534b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 훈련 시작...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, dataloader, sp, device)\u001b[39m\n\u001b[32m     32\u001b[39m optimizer.zero_grad()\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 순전파 (Forward Pass)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# model의 forward() 메서드 인자명과 일치시켜야 합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[32m     43\u001b[39m loss = criterion(predictions.view(-\u001b[32m1\u001b[39m, predictions.shape[-\u001b[32m1\u001b[39m]), tgt_output.reshape(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mTransformerModel.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m순전파 함수. 입력(src)과 타겟(tgt)을 받아 출력을 계산합니다.\u001b[39;00m\n\u001b[32m     55\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m \u001b[33;03m    tgt_padding_mask (Tensor): 타겟 패딩 마스크.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 1. 입력 시퀀스에 임베딩과 위치 인코딩 적용\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# 임베딩 값에 루트(d_model)을 곱하여 스케일링을 합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m src = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m * torch.sqrt(torch.tensor(\u001b[38;5;28mself\u001b[39m.transformer.d_model, dtype=torch.float32))\n\u001b[32m     67\u001b[39m src = \u001b[38;5;28mself\u001b[39m.positional_encoding(src)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# 2. 타겟 시퀀스에 임베딩과 위치 인코딩 적용\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# 수정된 create_masks 함수: 마스크를 입력 텐서와 동일한 장치로 이동시킵니다.\n",
    "def create_masks(src, tgt, sp):\n",
    "    # 인코더의 입력(src)에 대한 패딩 마스크 생성\n",
    "    # 패딩 토큰(sp.pad_id())의 위치를 True로 표시하여 어텐션 계산에서 제외합니다.\n",
    "    src_padding_mask = (src == sp.pad_id())\n",
    "\n",
    "    # 디코더의 입력(tgt)에 대한 미래 토큰 마스크 생성\n",
    "    # 디코더가 현재 단어 이후의 단어를 보지 못하도록 가립니다.\n",
    "    tgt_len = tgt.shape[1]\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
    "\n",
    "    # 디코더의 입력(tgt)에 대한 패딩 마스크 생성\n",
    "    tgt_padding_mask = (tgt == sp.pad_id())\n",
    "\n",
    "    # 반환하는 마스크는 3개입니다.\n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# 수정된 train_epoch 함수\n",
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        # 수정된 create_masks 함수를 호출하여 3개의 마스크를 받습니다.\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 (Forward Pass)\n",
    "        # model의 forward() 메서드 인자명과 일치시켜야 합니다.\n",
    "        predictions = model(src, tgt_input,\n",
    "                            src_mask=None,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 전체 훈련 루프\n",
    "NUM_EPOCHS = 10 \n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039659ea-d8ec-4a14-9ca5-2fcea256ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU로 해서 자꾸 에러가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d992068b-273e-4685-a02b-8f49e649c228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 장치: cpu\n",
      "모델 훈련 시작...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, dataloader, sp, device)\u001b[39m\n\u001b[32m     69\u001b[39m tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n\u001b[32m     70\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m                    \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m loss = criterion(predictions.view(-\u001b[32m1\u001b[39m, predictions.shape[-\u001b[32m1\u001b[39m]), tgt_output.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m     77\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mTransformerModel.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[39m\n\u001b[32m     25\u001b[39m tgt = \u001b[38;5;28mself\u001b[39m.embedding(tgt) * math.sqrt(\u001b[38;5;28mself\u001b[39m.transformer.d_model)\n\u001b[32m     26\u001b[39m tgt = \u001b[38;5;28mself\u001b[39m.positional_encoding(tgt)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                         \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                         \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m output = \u001b[38;5;28mself\u001b[39m.fc(output)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:270\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m src.size(-\u001b[32m1\u001b[39m) != \u001b[38;5;28mself\u001b[39m.d_model \u001b[38;5;129;01mor\u001b[39;00m tgt.size(-\u001b[32m1\u001b[39m) != \u001b[38;5;28mself\u001b[39m.d_model:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    267\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m memory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m output = \u001b[38;5;28mself\u001b[39m.decoder(\n\u001b[32m    277\u001b[39m     tgt,\n\u001b[32m    278\u001b[39m     memory,\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m     memory_is_causal=memory_is_causal,\n\u001b[32m    285\u001b[39m )\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:514\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    511\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    522\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:916\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    913\u001b[39m         x\n\u001b[32m    914\u001b[39m         + \u001b[38;5;28mself\u001b[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001b[32m    915\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:941\u001b[39m, in \u001b[36mTransformerEncoderLayer._ff_block\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.linear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# TransformerModel 클래스 (수정 없음)\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        src = self.embedding(src) * math.sqrt(self.transformer.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.transformer.d_model)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, \n",
    "                                 src_mask=src_mask, \n",
    "                                 tgt_mask=tgt_mask, \n",
    "                                 src_key_padding_mask=src_padding_mask, \n",
    "                                 tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 마스크 생성 함수 (수정 없음)\n",
    "def create_masks(src, tgt, sp):\n",
    "    src_padding_mask = (src == sp.pad_id()).float()\n",
    "    tgt_len = tgt.shape[1]\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
    "    tgt_padding_mask = (tgt == sp.pad_id()).float()\n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# 훈련 함수 (수정 없음)\n",
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(src, tgt_input,\n",
    "                            src_mask=None,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask)\n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# CPU 사용으로 변경: torch.cuda.is_available() 검사를 제거하고 'cpu'로 고정\n",
    "device = torch.device('cpu')\n",
    "print(f\"현재 사용 가능한 장치: {device}\")\n",
    "\n",
    "# 모델 인스턴스 생성 및 장치로 이동\n",
    "model = TransformerModel(\n",
    "    vocab_size=8000, \n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 정의\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "\n",
    "# 훈련 루프 실행\n",
    "NUM_EPOCHS = 10 \n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616eb3aa-01c1-4f35-8863-150d5f177338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러가 결려서 에러를 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d65b825-75c0-4fdc-81c6-0b76ae135016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 장치: cpu\n",
      "모델 훈련 시작...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 149\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 시작...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m모델 훈련 완료.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, dataloader, sp, device, log_interval)\u001b[39m\n\u001b[32m     73\u001b[39m predictions = model(src, tgt_input,\n\u001b[32m     74\u001b[39m                     src_mask=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     75\u001b[39m                     tgt_mask=tgt_mask,\n\u001b[32m     76\u001b[39m                     src_padding_mask=src_padding_mask,\n\u001b[32m     77\u001b[39m                     tgt_padding_mask=tgt_padding_mask)\n\u001b[32m     79\u001b[39m loss = criterion(predictions.view(-\u001b[32m1\u001b[39m, predictions.shape[-\u001b[32m1\u001b[39m]), tgt_output.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m optimizer.step()\n\u001b[32m     83\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# TransformerModel 클래스 (이전 수정사항 모두 반영)\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(src.device))\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(tgt.device))\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, \n",
    "                                 src_mask=src_mask, \n",
    "                                 tgt_mask=tgt_mask, \n",
    "                                 src_key_padding_mask=src_padding_mask, \n",
    "                                 tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "def create_masks(src, tgt, sp):\n",
    "    src_padding_mask = (src == sp.pad_id()).float()\n",
    "    tgt_len = tgt.shape[1]\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
    "    tgt_padding_mask = (tgt == sp.pad_id()).float()\n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # enumerate를 사용하여 배치 인덱스를 가져옵니다.\n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(src, tgt_input,\n",
    "                            src_mask=None,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask)\n",
    "\n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # [수정] Step별 Loss와 Acc를 계산하고 출력하는 코드\n",
    "        if i % log_interval == 0 and i != 0:\n",
    "            # Loss 계산\n",
    "            current_loss = loss.item()\n",
    "\n",
    "            # Accuracy (정확도) 계산\n",
    "            # topk를 사용하여 예측이 가장 높은 토큰을 찾습니다.\n",
    "            _, predicted_tokens = torch.max(predictions, dim=-1)\n",
    "            correct_predictions = (predicted_tokens == tgt_output)\n",
    "            \n",
    "            # 패딩 토큰은 정확도 계산에서 제외합니다.\n",
    "            # ignore_index가 -100이므로, 0을 제외하도록 수정\n",
    "            # (SentencePiece의 pad_id()는 0입니다)\n",
    "            non_pad_elements = (tgt_output != sp.pad_id())\n",
    "            correct_predictions_non_pad = correct_predictions[non_pad_elements]\n",
    "            \n",
    "            accuracy = correct_predictions_non_pad.sum().item() / non_pad_elements.sum().item()\n",
    "            \n",
    "            print(f\"[Epoch {epoch}, Step {i}] Loss: {current_loss:.4f}, Acc: {accuracy:.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# 데이터셋 크기 제한\n",
    "MAX_SAMPLES = 50000\n",
    "\n",
    "# 기존 데이터셋 로드\n",
    "# 여기서는 예시로 DataLoader를 직접 생성합니다. 실제 코드에 맞게 수정해주세요.\n",
    "# 예시: train_data, test_data = get_data() 또는 TensorDataset()\n",
    "# (아래 두 줄은 사용자님의 실제 데이터셋 로딩 코드로 대체해야 합니다.)\n",
    "# train_data = ...\n",
    "# test_data = ...\n",
    "\n",
    "# 데이터셋의 앞부분만 잘라내기\n",
    "# len() 함수를 사용하여 전체 데이터셋의 길이를 확인합니다.\n",
    "if 'train_data' in locals() and len(train_data) > MAX_SAMPLES:\n",
    "    train_data = train_data[:MAX_SAMPLES]\n",
    "\n",
    "# DataLoader 재정의 (이 부분은 사용자의 기존 코드에 맞게 수정해주세요)\n",
    "# 예시: train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# 예시: test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# CPU 사용으로 변경\n",
    "device = torch.device('cpu')\n",
    "print(f\"현재 사용 가능한 장치: {device}\")\n",
    "\n",
    "# 모델 인스턴스 생성 및 장치로 이동\n",
    "model = TransformerModel(\n",
    "    vocab_size=8000, \n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "\n",
    "# 훈련 루프 실행\n",
    "NUM_EPOCHS = 10 \n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8e8806d-123f-4482-b6bd-f2fa96c31eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 장치: cuda\n",
      "모델 훈련 시작...\n",
      "Epoch [1/20], Loss: 6.1058\n",
      "Epoch [2/20], Loss: 5.5136\n",
      "Epoch [3/20], Loss: 5.2292\n",
      "Epoch [4/20], Loss: 4.9403\n",
      "Epoch [5/20], Loss: 4.6626\n",
      "Epoch [6/20], Loss: 4.4011\n",
      "Epoch [7/20], Loss: 4.1543\n",
      "Epoch [8/20], Loss: 3.9167\n",
      "Epoch [9/20], Loss: 3.6896\n",
      "Epoch [10/20], Loss: 3.4721\n",
      "Epoch [11/20], Loss: 3.2600\n",
      "Epoch [12/20], Loss: 3.0719\n",
      "Epoch [13/20], Loss: 2.8944\n",
      "Epoch [14/20], Loss: 2.7250\n",
      "Epoch [15/20], Loss: 2.5715\n",
      "Epoch [16/20], Loss: 2.4247\n",
      "Epoch [17/20], Loss: 2.2991\n",
      "Epoch [18/20], Loss: 2.1858\n",
      "Epoch [19/20], Loss: 2.0794\n",
      "Epoch [20/20], Loss: 1.9870\n",
      "모델 훈련 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math\n",
    "\n",
    "# TransformerModel 클래스\n",
    "# forward 메서드 내부에 새로 생성되는 텐서를 .to(src.device)로 GPU에 옮기는 코드가 포함되어 있습니다.\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(src.device))\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(tgt.device))\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, \n",
    "                                 src_mask=src_mask, \n",
    "                                 tgt_mask=tgt_mask, \n",
    "                                 src_key_padding_mask=src_padding_mask, \n",
    "                                 tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "def create_masks(src, tgt, sp):\n",
    "    src_padding_mask = (src == sp.pad_id()).float()\n",
    "    tgt_len = tgt.shape[1]\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
    "    tgt_padding_mask = (tgt == sp.pad_id()).float()\n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 수정된 부분: model의 forward() 메서드 인자명에 맞게 변경\n",
    "        predictions = model(src, tgt_input,\n",
    "                            src_mask=None,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask)\n",
    "                            \n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# 데이터셋 크기 제한 (이전 단계와 동일)\n",
    "MAX_SAMPLES = 50000\n",
    "# 여기서는 예시로 DataLoader를 직접 생성합니다. 실제 코드에 맞게 수정해주세요.\n",
    "# 예시: train_data, test_data = get_data() 또는 TensorDataset()\n",
    "# (아래 두 줄은 사용자님의 실제 데이터셋 로딩 코드로 대체해야 합니다.)\n",
    "# train_data = ...\n",
    "# test_data = ...\n",
    "\n",
    "if 'train_data' in locals() and len(train_data) > MAX_SAMPLES:\n",
    "    train_data = train_data[:MAX_SAMPLES]\n",
    "\n",
    "# DataLoader 재정의 (이 부분은 사용자의 기존 코드에 맞게 수정해주세요)\n",
    "# 예시: train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# 예시: test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# 1. CUDA 사용 가능 여부 확인 후 'device' 변수 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"현재 사용 가능한 장치: {device}\")\n",
    "\n",
    "# 2. 모델 인스턴스 생성 및 장치로 이동\n",
    "model = TransformerModel(\n",
    "    vocab_size=8000, \n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "\n",
    "# 훈련 루프 실행\n",
    "NUM_EPOCHS = 20 \n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5a76409-6fe5-498e-9223-c56cb430c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 챗봇 추론 함수 정의하기\n",
    "\n",
    "def sentence_generation(model, sentence, sp, device, max_len=50):\n",
    "    # 훈련 모드를 비활성화하고 평가 모드로 전환\n",
    "    model.eval()\n",
    "    \n",
    "    # 입력 문장을 토큰 ID로 변환\n",
    "    tokens = sp.encode_as_ids(sentence)\n",
    "    \n",
    "    # [START] 토큰을 문장 앞에 추가\n",
    "    input_ids = [sp.bos_id()] + tokens\n",
    "    \n",
    "    # 텐서로 변환하고 디바이스로 이동\n",
    "    src_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 타겟 시퀀스 초기화: [START] 토큰으로 시작\n",
    "    tgt_tensor = torch.tensor([sp.bos_id()]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 최대 길이까지 반복하며 단어 생성\n",
    "    for _ in range(max_len):\n",
    "        # 마스크 생성 (디코더는 미래의 토큰을 보지 못함)\n",
    "        tgt_len = tgt_tensor.shape[1]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        # 수정된 부분: model의 forward() 메서드 인자명에 맞게 변경\n",
    "        predictions = model(src_tensor, tgt_tensor, src_mask=None, tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=None,\n",
    "                            tgt_padding_mask=None)\n",
    "                            \n",
    "        # 마지막 예측 토큰의 인덱스를 가져옴\n",
    "        last_prediction = predictions[:, -1, :]\n",
    "        \n",
    "        # 가장 높은 확률의 다음 토큰을 선택\n",
    "        _, next_token = torch.max(last_prediction, dim=-1)\n",
    "        next_token = next_token.item()\n",
    "        \n",
    "        # 예측된 토큰을 타겟 시퀀스에 추가\n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "        \n",
    "        # [END] 토큰이 예측되면 생성을 중단\n",
    "        if next_token == sp.eos_id():\n",
    "            break\n",
    "            \n",
    "    # 생성된 타겟 시퀀스(ID)를 다시 문장으로 변환\n",
    "    generated_sentence_ids = tgt_tensor.squeeze().tolist()\n",
    "    \n",
    "    # 불필요한 토큰([START], [END], [PAD]) 제거 후 문장으로 디코딩\n",
    "    generated_sentence = sp.decode(generated_sentence_ids)\n",
    "    \n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d93b0ef-f097-4645-9299-dafe786d2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 좋은 곳으로 데려다 줄 거예요.\n",
      "입력 : 어디에 있었어?\n",
      "출력 : 마음이 허전하겠어요.\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 테스트 실행하기 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.\n",
    "# 기존에 정의한 model, sp, device 변수를 사용합니다.\n",
    "# model = ...\n",
    "# sp = ...\n",
    "# device = ...\n",
    "\n",
    "# 테스트할 문장\n",
    "sentence = \"오늘 날씨 어때?\"\n",
    "print(f\"입력 : {sentence}\")\n",
    "\n",
    "# 챗봇에게 질문하고 응답 받기\n",
    "response = sentence_generation(model, sentence, sp, device)\n",
    "print(f\"출력 : {response}\")\n",
    "\n",
    "# 다른 문장으로 테스트\n",
    "sentence = \"어디에 있었어?\"\n",
    "print(f\"입력 : {sentence}\")\n",
    "response = sentence_generation(model, sentence, sp, device)\n",
    "print(f\"출력 : {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece18ac-5aa8-43a1-b466-94fd17102b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역시 동문 서답...\n",
    "# 데이터를 좀더 늘려서 진행해 보자 그리고 에폭도 늘려 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d65ee5f-c9a9-40bc-8b9d-4847aff99b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 가능한 장치: cuda\n",
      "모델 훈련 시작...\n",
      "Epoch [1/30], Loss: 6.1001\n",
      "Epoch [2/30], Loss: 5.5243\n",
      "Epoch [3/30], Loss: 5.2421\n",
      "Epoch [4/30], Loss: 4.9662\n",
      "Epoch [5/30], Loss: 4.6924\n",
      "Epoch [6/30], Loss: 4.4299\n",
      "Epoch [7/30], Loss: 4.1767\n",
      "Epoch [8/30], Loss: 3.9381\n",
      "Epoch [9/30], Loss: 3.7099\n",
      "Epoch [10/30], Loss: 3.4950\n",
      "Epoch [11/30], Loss: 3.2885\n",
      "Epoch [12/30], Loss: 3.0925\n",
      "Epoch [13/30], Loss: 2.9141\n",
      "Epoch [14/30], Loss: 2.7436\n",
      "Epoch [15/30], Loss: 2.5872\n",
      "Epoch [16/30], Loss: 2.4379\n",
      "Epoch [17/30], Loss: 2.3113\n",
      "Epoch [18/30], Loss: 2.1884\n",
      "Epoch [19/30], Loss: 2.0808\n",
      "Epoch [20/30], Loss: 1.9847\n",
      "Epoch [21/30], Loss: 1.8994\n",
      "Epoch [22/30], Loss: 1.8199\n",
      "Epoch [23/30], Loss: 1.7475\n",
      "Epoch [24/30], Loss: 1.6860\n",
      "Epoch [25/30], Loss: 1.6290\n",
      "Epoch [26/30], Loss: 1.5849\n",
      "Epoch [27/30], Loss: 1.5389\n",
      "Epoch [28/30], Loss: 1.5014\n",
      "Epoch [29/30], Loss: 1.4696\n",
      "Epoch [30/30], Loss: 1.4389\n",
      "모델 훈련 완료.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TransformerModel 클래스\n",
    "# forward 메서드 내부에 새로 생성되는 텐서를 .to(src.device)로 GPU에 옮기는 코드가 포함되어 있습니다.\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(src.device))\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.transformer.d_model, dtype=torch.float32).to(tgt.device))\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, \n",
    "                                 src_mask=src_mask, \n",
    "                                 tgt_mask=tgt_mask, \n",
    "                                 src_key_padding_mask=src_padding_mask, \n",
    "                                 tgt_key_padding_mask=tgt_padding_mask)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "def create_masks(src, tgt, sp):\n",
    "    src_padding_mask = (src == sp.pad_id()).float()\n",
    "    tgt_len = tgt.shape[1]\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(src.device)\n",
    "    tgt_padding_mask = (tgt == sp.pad_id()).float()\n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, dataloader, sp, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input, sp)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 수정된 부분: model의 forward() 메서드 인자명에 맞게 변경\n",
    "        predictions = model(src, tgt_input,\n",
    "                            src_mask=None,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask)\n",
    "                            \n",
    "        loss = criterion(predictions.view(-1, predictions.shape[-1]), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# 데이터셋 크기 제한 (이전 단계와 동일)\n",
    "MAX_SAMPLES = 100000\n",
    "# 여기서는 예시로 DataLoader를 직접 생성합니다. 실제 코드에 맞게 수정해주세요.\n",
    "# 예시: train_data, test_data = get_data() 또는 TensorDataset()\n",
    "# (아래 두 줄은 사용자님의 실제 데이터셋 로딩 코드로 대체해야 합니다.)\n",
    "# train_data = ...\n",
    "# test_data = ...\n",
    "\n",
    "if 'train_data' in locals() and len(train_data) > MAX_SAMPLES:\n",
    "    train_data = train_data[:MAX_SAMPLES]\n",
    "\n",
    "# DataLoader 재정의 (이 부분은 사용자의 기존 코드에 맞게 수정해주세요)\n",
    "# 예시: train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# 예시: test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# 1. CUDA 사용 가능 여부 확인 후 'device' 변수 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"현재 사용 가능한 장치: {device}\")\n",
    "\n",
    "# 2. 모델 인스턴스 생성 및 장치로 이동\n",
    "model = TransformerModel(\n",
    "    vocab_size=8000, \n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "\n",
    "# 훈련 루프 실행\n",
    "NUM_EPOCHS = 30 \n",
    "\n",
    "print(\"모델 훈련 시작...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, optimizer, criterion, train_dataloader, sp, device)\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ffee419-efcc-4188-af86-87939571289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 챗봇 추론 함수 정의하기\n",
    "\n",
    "def sentence_generation(model, sentence, sp, device, max_len=50):\n",
    "    # 훈련 모드를 비활성화하고 평가 모드로 전환\n",
    "    model.eval()\n",
    "    \n",
    "    # 입력 문장을 토큰 ID로 변환\n",
    "    tokens = sp.encode_as_ids(sentence)\n",
    "    \n",
    "    # [START] 토큰을 문장 앞에 추가\n",
    "    input_ids = [sp.bos_id()] + tokens\n",
    "    \n",
    "    # 텐서로 변환하고 디바이스로 이동\n",
    "    src_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 타겟 시퀀스 초기화: [START] 토큰으로 시작\n",
    "    tgt_tensor = torch.tensor([sp.bos_id()]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 최대 길이까지 반복하며 단어 생성\n",
    "    for _ in range(max_len):\n",
    "        # 마스크 생성 (디코더는 미래의 토큰을 보지 못함)\n",
    "        tgt_len = tgt_tensor.shape[1]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len).to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        # 수정된 부분: model의 forward() 메서드 인자명에 맞게 변경\n",
    "        predictions = model(src_tensor, tgt_tensor, src_mask=None, tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=None,\n",
    "                            tgt_padding_mask=None)\n",
    "                            \n",
    "        # 마지막 예측 토큰의 인덱스를 가져옴\n",
    "        last_prediction = predictions[:, -1, :]\n",
    "        \n",
    "        # 가장 높은 확률의 다음 토큰을 선택\n",
    "        _, next_token = torch.max(last_prediction, dim=-1)\n",
    "        next_token = next_token.item()\n",
    "        \n",
    "        # 예측된 토큰을 타겟 시퀀스에 추가\n",
    "        tgt_tensor = torch.cat([tgt_tensor, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "        \n",
    "        # [END] 토큰이 예측되면 생성을 중단\n",
    "        if next_token == sp.eos_id():\n",
    "            break\n",
    "            \n",
    "    # 생성된 타겟 시퀀스(ID)를 다시 문장으로 변환\n",
    "    generated_sentence_ids = tgt_tensor.squeeze().tolist()\n",
    "    \n",
    "    # 불필요한 토큰([START], [END], [PAD]) 제거 후 문장으로 디코딩\n",
    "    generated_sentence = sp.decode(generated_sentence_ids)\n",
    "    \n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0c3125f-89e4-4a43-ba0b-60381873f54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 저는 위로해드리는 로봇이에요.\n",
      "입력 : 어디에 있었어?\n",
      "출력 : 직접 물어보세요.\n"
     ]
    }
   ],
   "source": [
    "# 챗봇 테스트 실행하기 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.\n",
    "# 기존에 정의한 model, sp, device 변수를 사용합니다.\n",
    "# model = ...\n",
    "# sp = ...\n",
    "# device = ...\n",
    "\n",
    "# 테스트할 문장\n",
    "sentence = \"오늘 날씨 어때?\"\n",
    "print(f\"입력 : {sentence}\")\n",
    "\n",
    "# 챗봇에게 질문하고 응답 받기\n",
    "response = sentence_generation(model, sentence, sp, device)\n",
    "print(f\"출력 : {response}\")\n",
    "\n",
    "# 다른 문장으로 테스트\n",
    "sentence = \"어디에 있었어?\"\n",
    "print(f\"입력 : {sentence}\")\n",
    "response = sentence_generation(model, sentence, sp, device)\n",
    "print(f\"출력 : {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc372c-b618-4480-a754-be97373f6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여전히 동문 서답...\n",
    "# 뭐가 문제인가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
